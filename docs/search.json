[
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "About Me",
    "text": "About Me\n\n\n\n\n\n\nKatie Burak\nAssistant Professor of Teaching, Department of Statistics, UBC https://katieburak.github.io/"
  },
  {
    "objectID": "index.html#attribution",
    "href": "index.html#attribution",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Attribution",
    "text": "Attribution\n\nThis material is adapted from the following sources:\n\nDSCI 200 - Navigating Data: Acquisition, Exploration and Management, UBC ⭐️\nData Privacy Handbook\nDSCI 541: Privacy, Ethics, and Security at UBC\nHarvard University Privacy Tools Project: Differential Privacy\nSTA 199: Introduction to Data Science and Statistical Thinking, Duke University"
  },
  {
    "objectID": "index.html#what-are-you-comfortable-sharing",
    "href": "index.html#what-are-you-comfortable-sharing",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "What Are You Comfortable Sharing?",
    "text": "What Are You Comfortable Sharing?\n\nConsider different types of data:\n\n\nYour favorite type of music\n\nYour Instagram likes and follows\n\nYour e-mail\n\nYour name and DOB\n\nYour GPS location throughout the day\nYour browsing history\nYour private messages/DMs\n\n\n\n\n\n\nDiscussion:\n\n\nWhich of these data would you feel comfortable sharing with an app?\n\nWhat questions would you want to ask before sharing this data?\n\nWhat if it combined two or three pieces of information?"
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of today’s lesson, you should be able to:\n\n\nUnderstand key terms in data privacy, including PII, pseudonymization, and anonymization\nIdentify direct and indirect identifiers in sample data sets\nExplain why de-identification is challenging and context-dependent\nApply basic de-identification techniques (e.g., suppression, top-coding, permutation) using R\nRecognize the tradeoff between data utility and privacy risk"
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "",
    "text": "Even something as simple as your Facebook “likes” can reveal a lot more than you think…\nResearchers at Cambridge showed that algorithms could predict:\n\nSexual orientation with up to 88% accuracy\nRace with 95% accuracy\nPolitical affiliation with 85% accuracy\n\nAll from analyzing the pages and posts you “liked” (no profile bio or messages needed)!\n\n\nhttps://www.cam.ac.uk/research/news/digital-records-could-expose-intimate-details-and-personality-traits-of-millions"
  },
  {
    "objectID": "index.html#what-happens-to-your-data",
    "href": "index.html#what-happens-to-your-data",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "What Happens to Your Data?",
    "text": "What Happens to Your Data?\nEvery time you use an app, visit a website, click on a link, fill out a survey or even just scroll on your device, your data is being:\n\n\nCollected - What you click, search, watch, like or buy\n\nAnalyzed - Used to predict your behaviour, interests or identity\n\nShared or Sold - Passed to advertisers, data brokers or other companies"
  },
  {
    "objectID": "index.html#why-does-this-matter",
    "href": "index.html#why-does-this-matter",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Why Does This Matter?",
    "text": "Why Does This Matter?\n\n\nYou may be targeted with ads, content and potentially misinformation\n\nYou could be judged or profiled based on your data (even if it’s not accurate)\nYou rarely know who has your data (or what they’re doing with it)  \nSo what does this mean for us? Let’s explore how data can be used, what makes certain information sensitive and why it matters."
  },
  {
    "objectID": "index.html#personally-identifiable-information-pii",
    "href": "index.html#personally-identifiable-information-pii",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Personally Identifiable Information (PII)",
    "text": "Personally Identifiable Information (PII)\n\nPII refers to any data that can be used to identify a specific individual.\nDirect identifiers: These clearly and uniquely point to a person.\n\nExamples: name, social security number, patient ID\n\nIndirect identifiers: These don’t identify someone on their own, but could when combined.\n\nExamples: age, DOB, postal code, race, sex"
  },
  {
    "objectID": "index.html#personal-data",
    "href": "index.html#personal-data",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Personal Data",
    "text": "Personal Data\nData can be identifiable when:\n\n\nThey contain directly identifying information.\nIt’s possible to single out an individual\nIt’s possible to infer information about an individual based on information in your dataset\nIt’s possible to link records relating to an individual.\nDe-identification is still reversible."
  },
  {
    "objectID": "index.html#scenario-can-this-data-identify-you",
    "href": "index.html#scenario-can-this-data-identify-you",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Scenario: Can This Data Identify You?",
    "text": "Scenario: Can This Data Identify You?\nA fitness app shares anonymized data with researchers. The dataset includes:\n\nStep count per day\nGeneral location (postal code)\nAge\nTime of day the user exercises\nHealth conditions\n\nSeparately, a publicly available dataset includes information from a local running club: names, age groups and 5K race times."
  },
  {
    "objectID": "index.html#the-mosaic-effect",
    "href": "index.html#the-mosaic-effect",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "The Mosaic Effect",
    "text": "The Mosaic Effect\n\nThe “Mosaic Effect” can happens when separate pieces of data, which alone don’t identify anyone, are combined from different sources to reveal personal information or identify an individual.\nIn 2000, 87% of the United States population was found to be identifiable using a combination of their ZIP code, gender and date of birth.\n\n\n\nhttps://dataprivacylab.org/projects/identifiability/paper1.pdf"
  },
  {
    "objectID": "index.html#pseudonymization-and-anonymization",
    "href": "index.html#pseudonymization-and-anonymization",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Pseudonymization and Anonymization",
    "text": "Pseudonymization and Anonymization\n\nPseudonymisation and a nonymisation are techniques to de-identify personal data\nGoal: reduce linkability of data to individuals\nWe will now define each of these terms"
  },
  {
    "objectID": "index.html#pseudonymization",
    "href": "index.html#pseudonymization",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Pseudonymization",
    "text": "Pseudonymization\n\n\nReduces linkability of data to individuals\nData cannot identify individuals without additional information\nOften done by replacing direct identifiers with pseudonyms\nLink between real identifiers and pseudonyms is stored separately\nRe-identification remains possible!"
  },
  {
    "objectID": "index.html#anonymization",
    "href": "index.html#anonymization",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Anonymization",
    "text": "Anonymization\n\n\nData are anonymized when no individual is identifiable (directly or indirectly)\nThis applies even to the data controller\nFully anonymized data are no longer personal data\nAnonymisation is difficult to achieve in practice"
  },
  {
    "objectID": "index.html#identifiability-spectrum",
    "href": "index.html#identifiability-spectrum",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Identifiability Spectrum",
    "text": "Identifiability Spectrum\n\nIdentifiability is a spectrum\nMore de-identified data = closer to anonymized\nLower identifiability = lower re-identification risk\n\n\n\nhttps://www.kdnuggets.com/2020/08/anonymous-anonymized-data.html"
  },
  {
    "objectID": "index.html#when-are-data-truly-anonymous",
    "href": "index.html#when-are-data-truly-anonymous",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "When Are Data Truly anonymous?",
    "text": "When Are Data Truly anonymous?\n\nOnly if re-identification would require unreasonable effort (factors include cost, time and available technology)\nData are not anonymous if:\n\n\n\nDirect identifiers are present\nIndividuals can be singled out from a group\nRe-identification possible via linking datasets (mosaic effect)\nInference about identity is possible (e.g., through different variables)\nDe-identification can be reversed"
  },
  {
    "objectID": "index.html#context-matters",
    "href": "index.html#context-matters",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Context Matters",
    "text": "Context Matters\n\nWhether data are anonymous depends on:\n\nThe context of the research\nAvailable external information\nFuture data uses"
  },
  {
    "objectID": "index.html#de-identification-techniques",
    "href": "index.html#de-identification-techniques",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "De-identification Techniques",
    "text": "De-identification Techniques\nTechniques to deidentify your data include:\n\nSuppression\nGeneralization\nReplacement\nTop- and bottom coding\nAdding noise\nPermutation\n\nWe will talk about each of these techniques individually."
  },
  {
    "objectID": "index.html#section-1",
    "href": "index.html#section-1",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "",
    "text": "First, let’s generate some data we can use to help illustrate these concepts.\n\n\n# A tibble: 4 × 3\n  name             age height_cm\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n1 Joel Miller       52       182\n2 Ellie Williams    19       160\n3 Tommy Miller      48       185\n4 Abby Anderson     28       173"
  },
  {
    "objectID": "index.html#suppression",
    "href": "index.html#suppression",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Suppression",
    "text": "Suppression\n\n\nRemove entire variables, values or records\nUsed to eliminate highly identifying or unnecessary data\nExamples:\n\nNames, contact details, social security numbers\nGPS metadata, IP addresses, neuroimaging facial features\nOutliers or unique participants"
  },
  {
    "objectID": "index.html#suppression-example",
    "href": "index.html#suppression-example",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Suppression Example",
    "text": "Suppression Example\n\n\n# A tibble: 4 × 2\n    age height_cm\n  &lt;dbl&gt;     &lt;dbl&gt;\n1    52       182\n2    19       160\n3    48       185\n4    28       173"
  },
  {
    "objectID": "index.html#generalization",
    "href": "index.html#generalization",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Generalization",
    "text": "Generalization\n\n\nReduces detail or granularity in the data\nMakes individuals harder to single out\nExamples:\n\nConvert date of birth to age, or group into ranges\nReplace address with town or region\nRecategorise rare labels into “other” or “missing”\nAbstract people or places in qualitative data (e.g., “Bob” to “[colleague]”)"
  },
  {
    "objectID": "index.html#generalization-example",
    "href": "index.html#generalization-example",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Generalization Example",
    "text": "Generalization Example\nHere we will show an example of generalization on the age column:\n\n\n# A tibble: 4 × 3\n  name           height_cm age_group\n  &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;    \n1 Joel Miller          182 30+      \n2 Ellie Williams       160 under 30 \n3 Tommy Miller         185 30+      \n4 Abby Anderson        173 under 30"
  },
  {
    "objectID": "index.html#replacement",
    "href": "index.html#replacement",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Replacement",
    "text": "Replacement\n\n\nSwap identifying info with less informative alternatives\nExamples:\n\nUse pseudonyms for names (with securely stored keyfile)\nReplace with placeholders (e.g., “[redacted]”)\nRounding numeric values"
  },
  {
    "objectID": "index.html#creating-pseudonyms",
    "href": "index.html#creating-pseudonyms",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Creating Pseudonyms",
    "text": "Creating Pseudonyms\n\n\nPseudonyms should reveal nothing about the subject\nGood pseudonyms:\n\nAre random or meaningless strings/numbers\nAre securely managed (e.g., encrypted keyfile)\n\nCan be generated using tools in Excel, R, Python, SPSS"
  },
  {
    "objectID": "index.html#replacement-with-pseudonyms",
    "href": "index.html#replacement-with-pseudonyms",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Replacement with Pseudonyms",
    "text": "Replacement with Pseudonyms\n\n\n# A tibble: 4 × 3\n  pseudonym   age height_cm\n  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 ID1          52       182\n2 ID2          19       160\n3 ID3          48       185\n4 ID4          28       173"
  },
  {
    "objectID": "index.html#hashing",
    "href": "index.html#hashing",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Hashing",
    "text": "Hashing\n\nHashing converts names into fixed-length, irreversible strings.\nUnlike pseudonyms, hashed values cannot be easily reversed.\nIn R, we can use the digest package (and function) to hash.\n\n\n\n# A tibble: 4 × 3\n# Rowwise: \n  name_hash                          age height_cm\n  &lt;chr&gt;                            &lt;dbl&gt;     &lt;dbl&gt;\n1 4a3e0ee26ab3fb1338e893f4d4e7244b    52       182\n2 201943dd66d423ed3cce2242a75736d4    19       160\n3 81699ec9483bad176eed57ee43ffa010    48       185\n4 046dff9ba9cf33573396f4de8c0c0e0b    28       173"
  },
  {
    "objectID": "index.html#section-2",
    "href": "index.html#section-2",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "",
    "text": "What happens if we juse apply digest to the name vector without using rowwise?\n\n\ndigest hashed the entire name column as a single object (it’s not vectorized), so mutate recycled the same hash to every row (which is not what we want)."
  },
  {
    "objectID": "index.html#top--and-bottom-coding",
    "href": "index.html#top--and-bottom-coding",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Top- and Bottom-Coding",
    "text": "Top- and Bottom-Coding\n\n\nLimits extreme values in quantitative data\nRecode all values above or below a threshold\nExample: all incomes above $150,000 become $150,000\nPreserves much of the dataset, but distorts distribution tails"
  },
  {
    "objectID": "index.html#top-coding-example",
    "href": "index.html#top-coding-example",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Top-coding example",
    "text": "Top-coding example\n\nConsider 6ft (182.88cm) is considered our maximum height threshold.\n\n\n\n# A tibble: 4 × 3\n  name             age height_cm\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n1 Joel Miller       52      182 \n2 Ellie Williams    19      160 \n3 Tommy Miller      48      183.\n4 Abby Anderson     28      173"
  },
  {
    "objectID": "index.html#adding-noise",
    "href": "index.html#adding-noise",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Adding Noise",
    "text": "Adding Noise\n\nIntroduces randomness to protect sensitive info\nExamples:\n\nAdd a small random amount to numeric values\nBlur images or alter voices\nUse differential privacy algorithms (advanced)"
  },
  {
    "objectID": "index.html#adding-noise-to-height",
    "href": "index.html#adding-noise-to-height",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Adding Noise to Height",
    "text": "Adding Noise to Height\nThis adds random noise to the height variable from a normal distribution (\\(\\mu=0\\), \\(\\sigma=2\\)), reducing exact re-identification risk.\n\n\n# A tibble: 4 × 3\n  name             age height_cm_noisy\n  &lt;chr&gt;          &lt;dbl&gt;           &lt;dbl&gt;\n1 Joel Miller       52            182.\n2 Ellie Williams    19            160.\n3 Tommy Miller      48            186.\n4 Abby Anderson     28            174."
  },
  {
    "objectID": "index.html#permutation",
    "href": "index.html#permutation",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Permutation",
    "text": "Permutation\n\n\nSwap values between individuals\nMakes linking variables across a record more difficult\nMaintains distributions, but breaks correlations\nCan limit the types of analyses possible"
  },
  {
    "objectID": "index.html#permutation-of-height-values",
    "href": "index.html#permutation-of-height-values",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Permutation of Height Values",
    "text": "Permutation of Height Values\nHere, the height_cm values are shuffled between individuals, preserving the overall distribution but breaking the link between person and value.\n\n\n# A tibble: 4 × 3\n  name             age height_cm_permuted\n  &lt;chr&gt;          &lt;dbl&gt;              &lt;dbl&gt;\n1 Joel Miller       52                160\n2 Ellie Williams    19                173\n3 Tommy Miller      48                182\n4 Abby Anderson     28                185"
  },
  {
    "objectID": "index.html#privacy-vs.-utility-tradeoff",
    "href": "index.html#privacy-vs.-utility-tradeoff",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Privacy vs. Utility Tradeoff",
    "text": "Privacy vs. Utility Tradeoff\n\n\nhttps://www.researchgate.net/figure/Trade-off-between-privacy-level-and-utility-level-of-data_fig1_357987903"
  },
  {
    "objectID": "index.html#key-takeaways",
    "href": "index.html#key-takeaways",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nData exists on a spectrum of identifiability\nEven seemingly anonymous data can often be re-identified (e.g., mosaic effect)\nDifferent techniques offer varying levels of protection and utility\nContext, external data and technological capabilities all affect re-identification risk\nResponsible data handling requires both technical skill and ethical awareness"
  },
  {
    "objectID": "index.html#case-study-brogan-inc.-and-nihb-data",
    "href": "index.html#case-study-brogan-inc.-and-nihb-data",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Case Study: Brogan Inc. and NIHB Data",
    "text": "Case Study: Brogan Inc. and NIHB Data\n\nThe Non-Insured Health Benefits (NIHB) database contains sensitive health data on First Nations use of services like prescriptions, dental care, and medical devices.\nIn 2001, Health Canada began releasing de-identified NIHB pharmacy claims data to Brogan Inc., a private health consulting firm.\nThough personal identifiers were removed, community identifiers remained, and First Nations were not informed until 2007.\nBrogan sold the data to pharmaceutical companies for commercial research and marketing\nHealth Canada justified the release by claiming no privacy interests remained since personally identifying information had been removed.\n\n\nKukutai, T., & Taylor, J. (2016). Indigenous data sovereignty: Toward an agenda. ANU press."
  },
  {
    "objectID": "index.html#discussion",
    "href": "index.html#discussion",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Discussion",
    "text": "Discussion\nTake 5 minutes to discuss this case in groups of 2-3. Consider these questions to reflect on:\n\nWas the data truly de-identified?\nShould de-identified data still require community consent before being shared or sold?\nWhat are the limits of simply removing names and IDs from a dataset?\nHow can we measure whether a dataset is truly “safe” to release?"
  },
  {
    "objectID": "index.html#learning-objectives-1",
    "href": "index.html#learning-objectives-1",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of today’s lesson, you should be able to:\n\nDefine identifiers, quasi-identifiers and sensitive attributes in data sets\n\nExplain the limitations of basic deidentification methods\n\nDescribe the concepts of \\(k\\)-anonymity, \\(l\\)-diversity and \\(t\\)-closeness\n\nApply \\(k\\)-anonymity and \\(l\\)-diversity to de-identify data\nUnderstand the basic idea of differential privacy and its significance"
  },
  {
    "objectID": "index.html#why-basic-deidentification-isnt-always-enough",
    "href": "index.html#why-basic-deidentification-isnt-always-enough",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Why basic deidentification isn’t always enough",
    "text": "Why basic deidentification isn’t always enough\n\nLast class, we introduced some techniques for deidentification such as suppression and generalization.\nHowever, individuals can often be re-identified using other information.\nAs datasets become more detailed and linkable, privacy risks increase.\nStatistical methods are needed to ensure meaningful deidentification while preserving data utility."
  },
  {
    "objectID": "index.html#statistical-approaches-to-deidentification",
    "href": "index.html#statistical-approaches-to-deidentification",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Statistical approaches to deidentification",
    "text": "Statistical approaches to deidentification\n\n\n\\(k\\)-anonymity\n\n\\(l\\)-diversity\n\n\\(t\\)-closeness\n\nDifferential privacy (advanced)"
  },
  {
    "objectID": "index.html#overview-of-privacy-models",
    "href": "index.html#overview-of-privacy-models",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Overview of privacy models",
    "text": "Overview of privacy models\n\n\\(k\\)-anonymity, \\(l\\)-diversity, and \\(t\\)-closeness are statistical approaches that quantify the level of identifiability within a tabular dataset.\n\nThey focus on how variables combined can lead to identification.\n\nThese approaches are complementary: a dataset can be simultaneously \\(k\\)-anonymous, \\(l\\)-diverse, and \\(t\\)-close, where \\(k\\), \\(l\\), and \\(t\\) represent numeric thresholds.\n\\(k\\)-anonymity, \\(l\\)-diversity, and \\(t\\)-closeness are typically used to de-identify tabular datasets before sharing.\nThey work best on relatively large datasets, where enough observations are present to preserve useful detail while still protecting privacy."
  },
  {
    "objectID": "index.html#identifiers-quasi-identifiers-and-sensitive-attributes",
    "href": "index.html#identifiers-quasi-identifiers-and-sensitive-attributes",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Identifiers, Quasi-Identifiers, and Sensitive Attributes",
    "text": "Identifiers, Quasi-Identifiers, and Sensitive Attributes\nPrivacy models distinguish between three types of variables:\n\nIdentifiers: Direct identifiers such as names, student numbers, email addresses.\nQuasi-Identifiers: Indirect identifiers that can lead to identification when combined with other quasi-identifiers or external data.\n\nExamples: age, sex, place of residence, physical characteristics, timestamps, etc.\n\nSensitive Attributes: Variables of interest that need protection and cannot be altered as they are key outcomes.\n\nExamples: Medical condition, Income, etc."
  },
  {
    "objectID": "index.html#importance-of-correct-variable-categorization",
    "href": "index.html#importance-of-correct-variable-categorization",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Importance of Correct Variable Categorization",
    "text": "Importance of Correct Variable Categorization\n\n\nCorrectly categorizing variables into identifiers, quasi-identifiers, and sensitive attributes is crucial.\n\nThis categorization determines how to de-identify your dataset effectively using \\(k\\)-anonymity, \\(l\\)-diversity, and \\(t\\)-closeness.\nNow, let’s discuss each of these techniques in detail…"
  },
  {
    "objectID": "index.html#k-anonymity",
    "href": "index.html#k-anonymity",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "\\(k\\)-anonymity",
    "text": "\\(k\\)-anonymity\n\n\nA data set is \\(k\\)-anonymous if each observation cannot be distinguished from at least \\(k-1\\) other observations based on the quasi-identifiers.\n\nThis can be achieved through generalization, suppression and sometimes top- or bottom-coding of data values.\nApplying \\(k\\)-anonymity makes it more difficult for an attacker to single out or re-identify specific individuals.\n\nIt also helps reduce the risk of the mosaic effect, where combining data points could lead to identification."
  },
  {
    "objectID": "index.html#making-a-data-set-k-anonymous",
    "href": "index.html#making-a-data-set-k-anonymous",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Making a data set \\(k\\)-anonymous",
    "text": "Making a data set \\(k\\)-anonymous\n\n\nIdentify variables as identifiers, quasi-identifiers and sensitive attributes.\n\nChoose a value for \\(k\\).\n\nAggregate or transform the data so each combination of quasi-identifiers occurs at least k times."
  },
  {
    "objectID": "index.html#choosing-k",
    "href": "index.html#choosing-k",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Choosing \\(k\\)",
    "text": "Choosing \\(k\\)\n\nThere is no single correct value for \\(k\\)!\nHigher \\(k\\) increases privacy, but reduces data detail and utility.\n\nThe choice depends on promises made to data subjects and acceptable risk levels.\n\n\n\n Source: k2view.com"
  },
  {
    "objectID": "index.html#example-data",
    "href": "index.html#example-data",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Example data",
    "text": "Example data\n\nAge and city are quasi-identifiers, and salary is considered a sensitive attribute.\n\n\n\n\n\nAge\n\n\nCity\n\n\nSalary\n\n\n\n\n\n\n38\n\n\nCalgary\n\n\n91,000\n\n\n\n\n37\n\n\nToronto\n\n\n92,000\n\n\n\n\n31\n\n\nVancouver\n\n\n82,000\n\n\n\n\n48\n\n\nCalgary\n\n\n115,000\n\n\n\n\n39\n\n\nVancouver\n\n\n118,000\n\n\n\n\n37\n\n\nCalgary\n\n\n97,000\n\n\n\n\n34\n\n\nToronto\n\n\n98,000\n\n\n\n\n33\n\n\nVancouver\n\n\n89,000\n\n\n\n\n32\n\n\nToronto\n\n\n108,000\n\n\n\n\n45\n\n\nCalgary\n\n\n95,000"
  },
  {
    "objectID": "index.html#k2",
    "href": "index.html#k2",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "\\(k=2\\)",
    "text": "\\(k=2\\)\n\n\n\n\nAge Range\n\n\nCity\n\n\nSalary Range\n\n\n\n\n\n\n30–39\n\n\nCalgary\n\n\n90,000–99,999\n\n\n\n\n30–39\n\n\nToronto\n\n\n90,000–99,999\n\n\n\n\n30–39\n\n\nVancouver\n\n\n80,000–89,999\n\n\n\n\n40–49\n\n\nCalgary\n\n\n110,000–119,999\n\n\n\n\n30–39\n\n\nVancouver\n\n\n110,000–119,999\n\n\n\n\n30–39\n\n\nCalgary\n\n\n90,000–99,999\n\n\n\n\n30–39\n\n\nToronto\n\n\n90,000–99,999\n\n\n\n\n30–39\n\n\nVancouver\n\n\n80,000–89,999\n\n\n\n\n30–39\n\n\nToronto\n\n\n100,000–109,999\n\n\n\n\n40–49\n\n\nCalgary\n\n\n90,000–99,999"
  },
  {
    "objectID": "index.html#l-diversity",
    "href": "index.html#l-diversity",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "\\(l\\)-diversity",
    "text": "\\(l\\)-diversity\n\n\\(l\\)-diversity is an extension of \\(k\\)-anonymity that ensures sufficient variation in a sensitive attribute.\nThis is important because if all individuals within a group share the same sensitive value, there is still a risk of inference."
  },
  {
    "objectID": "index.html#section-3",
    "href": "index.html#section-3",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "",
    "text": "Although these data are \\(2\\)-anonymous, we can still infer that any 30-39 year old from Calgary who participated earns between 90-99k.\n\n\n\n\n\nAge Range\n\n\nCity\n\n\nSalary Range\n\n\n\n\n\n\n30–39\n\n\nCalgary\n\n\n90,000–99,999\n\n\n\n\n30–39\n\n\nToronto\n\n\n90,000–99,999\n\n\n\n\n30–39\n\n\nVancouver\n\n\n80,000–89,999\n\n\n\n\n40–49\n\n\nCalgary\n\n\n110,000–119,999\n\n\n\n\n30–39\n\n\nVancouver\n\n\n110,000–119,999\n\n\n\n\n30–39\n\n\nCalgary\n\n\n90,000–99,999\n\n\n\n\n30–39\n\n\nToronto\n\n\n90,000–99,999\n\n\n\n\n30–39\n\n\nVancouver\n\n\n80,000–89,999\n\n\n\n\n30–39\n\n\nToronto\n\n\n100,000–109,999\n\n\n\n\n40–49\n\n\nCalgary\n\n\n90,000–99,999"
  },
  {
    "objectID": "index.html#l-diversity-1",
    "href": "index.html#l-diversity-1",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "\\(l\\)-diversity",
    "text": "\\(l\\)-diversity\n\nThe approach requires at least \\(l\\) different values for the sensitive attribute within each combination of quasi-identifiers.\nAgain, there is no perfect value for \\(l\\) (typically \\(1&lt; l \\leq k\\))."
  },
  {
    "objectID": "index.html#section-4",
    "href": "index.html#section-4",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "",
    "text": "With \\(l=2\\), that means that for each combination of Age Range and City, there are at least 2 distinct Salary Ranges.\n\n\n\n\n\nAge Range\n\n\nCity\n\n\nSalary Range\n\n\n\n\n\n\n30–39\n\n\n-\n\n\n90,000–99,999\n\n\n\n\n30–39\n\n\n-\n\n\n90,000–99,999\n\n\n\n\n30–39\n\n\n-\n\n\n80,000–89,999\n\n\n\n\n40–49\n\n\nCalgary\n\n\n110,000–119,999\n\n\n\n\n30–39\n\n\n-\n\n\n110,000–119,999\n\n\n\n\n30–39\n\n\n-\n\n\n90,000–99,999\n\n\n\n\n30–39\n\n\n-\n\n\n90,000–99,999\n\n\n\n\n30–39\n\n\n-\n\n\n80,000–89,999\n\n\n\n\n30–39\n\n\n-\n\n\n100,000–109,999\n\n\n\n\n40–49\n\n\nCalgary\n\n\n90,000–99,999"
  },
  {
    "objectID": "index.html#t-closeness",
    "href": "index.html#t-closeness",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "\\(t\\)-closeness",
    "text": "\\(t\\)-closeness\n\n\n\\(t\\)-closeness builds on k-anonymity and l-diversity by requiring that the distribution of the sensitive attribute within each group of quasi-identifiers is close to the distribution in the full dataset.\nThis prevents situations where a sensitive value is overly dominant in a group, which could allow re-identification through skewed distributions.\nFor example, in a dataset with Age and Sex as quasi-identifiers and Income as the sensitive attribute, applying t-closeness with \\(t = 0.1\\) means the income distribution in each group must stay within 10% of the overall income distribution.\nIn this course, we will focus mainly on \\(k\\)-anonymity and \\(l\\)-diversity as \\(t\\)-closeness can get complicated to implement."
  },
  {
    "objectID": "index.html#there-are-still-issues",
    "href": "index.html#there-are-still-issues",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "There are still issues…",
    "text": "There are still issues…\n\nEven though the data is de-identified, some sensitive patterns can still leak through.\nIn the example we discussed, both individuals are grouped into the same age range and city.\nWhile they are in different salary ranges and exact values are hidden, the range is still quite narrow.\nDue to the similarity of the salary ranges, one can still infer that both individuals earn between $90,000 and $119,999.\n\n\n\n\n\nAge Range\n\n\nCity\n\n\nSalary Range\n\n\n\n\n\n\n40–49\n\n\nCalgary\n\n\n110,000–119,999\n\n\n\n\n40–49\n\n\nCalgary\n\n\n90,000–99,999"
  },
  {
    "objectID": "index.html#differential-privacy",
    "href": "index.html#differential-privacy",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Differential privacy",
    "text": "Differential privacy\n\nSo, we may need more sophisticated tools to privatize our data…\nDifferential privacy is a mathematical approach to protecting privacy\nIt ensures algorithm results are nearly the same whether one person’s data is included or not\nDifferential privacy makes it hard to tell if any individual’s data is in the dataset, which protects individual’s information (even with unusual or unique data)"
  },
  {
    "objectID": "index.html#section-5",
    "href": "index.html#section-5",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "",
    "text": "Source: https://medium.com/data-science/a-differential-privacy-example-for-beginners-ef3c23f69401"
  },
  {
    "objectID": "index.html#section-6",
    "href": "index.html#section-6",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "",
    "text": "Differential privacy is a complex topic and goes beyond the scope of this course\n\nFor a clear and accessible explanation, check out this short video:"
  },
  {
    "objectID": "index.html#iclicker-question-1",
    "href": "index.html#iclicker-question-1",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "iClicker Question 1",
    "text": "iClicker Question 1\nGiven the data, which field(s) could you generalize to help achieve k = 3 anonymity?\n\n\n\n\nAge\nZIP Code\nDisease\n\n\n\n\n29\n13053\nFlu\n\n\n27\n13068\nFlu\n\n\n28\n13068\nCold\n\n\n45\n14853\nDiabetes\n\n\n46\n14853\nDiabetes\n\n\n47\n14853\nCancer\n\n\n\n\n\nA. Generalize Age into age ranges (e.g., 20–29, 40–49)\n\nB. Suppress Disease entirely\n\nC. Generalize ZIP Code to first 3 digits (e.g., 130, 148)\n\nD. Generalize Age into age ranges (e.g., 20–29, 40–49) and ZIP code to first 3 digits (e.g., 130, 148)\n\nE. It’s already \\(k=3\\) anonymous"
  },
  {
    "objectID": "index.html#iclicker-question-2",
    "href": "index.html#iclicker-question-2",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "iClicker Question 2",
    "text": "iClicker Question 2\nWhich of the following datasets violates \\(k = 2\\) anonymity?\n\n\nOption A\n\n\n\nAge\nSex\nZIP\n\n\n\n\n34\nM\n02138\n\n\n34\nM\n02138\n\n\n34\nF\n02139\n\n\n\n\nOption B\n\n\n\nAge\nSex\nZIP\n\n\n\n\n22\nF\n10011\n\n\n22\nF\n10011\n\n\n22\nF\n10011\n\n\n\n\nOption C\n\n\n\nAge Range\nSex\nZIP Prefix\n\n\n\n\n30–39\n*\n021**\n\n\n30–39\n*\n021**\n\n\n30–39\n*\n021**\n\n\n\n\n\nA. Only A\n\nB. Only B\n\nC. Only C\n\nD. A and B"
  },
  {
    "objectID": "index.html#question-3",
    "href": "index.html#question-3",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Question 3",
    "text": "Question 3\nConsider this 3-anonymous dataset. Is it also 2-diverse with respect to “Condition”?\n\n\n\n\nAge Range\nZIP Prefix\nCondition\n\n\n\n\n20–29\n130**\nFlu\n\n\n20–29\n130**\nFlu\n\n\n20–29\n130**\nFlu\n\n\n30–39\n148**\nCold\n\n\n30–39\n148**\nCold\n\n\n30–39\n148**\nCancer\n\n\n\n\n\nA. Yes, both groups have 2 or more different values\n\nB. No, one group violates l-diversity\n\nC. Yes, because the dataset is already k-anonymous\n\nD. No, both groups have only one distinct value"
  },
  {
    "objectID": "index.html#key-takeaways-1",
    "href": "index.html#key-takeaways-1",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nRemoving direct identifiers alone does not guarantee privacy\n\nQuasi-identifiers can lead to re-identification if not protected\n\n\\(k\\)-anonymity makes each record indistinguishable from at least \\(k - 1\\) others\n\\(l\\)-diversity improves protection by promoting diversity in sensitive attributes\nDifferential privacy offers mathematical privacy guarantees\n\nChoosing privacy parameters involves balancing risk and data utility"
  },
  {
    "objectID": "index.html#section-7",
    "href": "index.html#section-7",
    "title": "An Introduction to Data Privacy in Practice",
    "section": "",
    "text": "Thank you! Questions?"
  }
]